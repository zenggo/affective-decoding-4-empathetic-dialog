{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from configs import DEFAULT_MODEL_CFG, EMOTION_CATES\n",
    "from model import ELMModel\n",
    "from indexer import Indexer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DEFAULT_MODEL_CFG\n",
    "indexer = Indexer(cfg.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tops(beta, model_path, k=200):\n",
    "    # load model\n",
    "    model = ELMModel(cfg, indexer.n_vocab, indexer.n_special, indexer.n_ctx, indexer, beta)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    # read weights\n",
    "    ES = model.ES.weight.data\n",
    "    VS = model.VS.weight.data\n",
    "    EL = model.EL.weight.data\n",
    "    VL = model.VL.weight.data\n",
    "    # calculate bias\n",
    "    bias_S = torch.matmul(VS, ES.T)\n",
    "    bias_L = torch.matmul(VL, EL.T)\n",
    "    # stat\n",
    "    res = {}\n",
    "    for i, emo in enumerate(EMOTION_CATES):\n",
    "        tmp = {}\n",
    "        # speaker\n",
    "        tmp['bias_mean_S'] = bias_S.mean().item()\n",
    "        tmp['bias_std_S'] = bias_S.std().item()\n",
    "        values, indices = bias_S[:, i].topk(k)\n",
    "        tmp['top_S'] = [(indexer.decode_index2text(idx), v) \\\n",
    "                        for v, idx in zip(values.tolist(), indices.tolist())]\n",
    "        # listener\n",
    "        tmp['bias_mean_L'] = bias_L.mean().item()\n",
    "        tmp['bias_std_L'] = bias_L.std().item()\n",
    "        values, indices = bias_L[:, i].topk(k)\n",
    "        tmp['top_L'] = [(indexer.decode_index2text(idx), v) \\\n",
    "                        for v, idx in zip(values.tolist(), indices.tolist())]\n",
    "        res[emo] = tmp\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = find_tops(1.0, 'save/elm_1')\n",
    "with open('save/stat_elm_1.json', 'w') as outfile:\n",
    "    json.dump(res_1, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = find_tops(2.0, 'save/elm_2')\n",
    "with open('save/stat_elm_2.json', 'w') as outfile:\n",
    "    json.dump(res_2, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3 = find_tops(3.0, 'save/elm_3')\n",
    "with open('save/stat_elm_3.json', 'w') as outfile:\n",
    "    json.dump(res_3, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1_S = 0.0\n",
    "mean_2_S = 0.0\n",
    "mean_3_S = 0.0\n",
    "mean_1_L = 0.0\n",
    "mean_2_L = 0.0\n",
    "mean_3_L = 0.0\n",
    "max_1_S = 0.0\n",
    "max_2_S = 0.0\n",
    "max_3_S = 0.0\n",
    "max_1_L = 0.0\n",
    "max_2_L = 0.0\n",
    "max_3_L = 0.0\n",
    "for e in EMOTION_CATES:\n",
    "    mean_1_S += res_1[e]['bias_mean_S']\n",
    "    mean_2_S += res_2[e]['bias_mean_S']\n",
    "    mean_3_S += res_3[e]['bias_mean_S']\n",
    "    mean_1_L += res_1[e]['bias_mean_L']\n",
    "    mean_2_L += res_2[e]['bias_mean_L']\n",
    "    mean_3_L += res_3[e]['bias_mean_L']\n",
    "    max_1_S += res_1[e]['top_S'][0][1]\n",
    "    max_2_S += res_2[e]['top_S'][0][1]\n",
    "    max_3_S += res_3[e]['top_S'][0][1]\n",
    "    max_1_L += res_1[e]['top_L'][0][1]\n",
    "    max_2_L += res_2[e]['top_L'][0][1]\n",
    "    max_3_L += res_3[e]['top_L'][0][1]\n",
    "mean_1_S /= 32\n",
    "mean_2_S /= 32\n",
    "mean_3_S /= 32\n",
    "mean_1_L /= 32\n",
    "mean_2_L /= 32\n",
    "mean_3_L /= 32\n",
    "max_1_S /= 32\n",
    "max_2_S /= 32\n",
    "max_3_S /= 32\n",
    "max_1_L /= 32\n",
    "max_2_L /= 32\n",
    "max_3_L /= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010458228178322315 -0.014138422906398773 -0.014582054689526558\n",
      "-0.010079051367938519 -0.014124294742941856 -0.014553489163517952\n",
      "0.06316951010376215 0.059953586431220174 0.058305080397985876\n",
      "0.05899635446257889 0.05655928107444197 0.05512049177195877\n"
     ]
    }
   ],
   "source": [
    "print(mean_1_S, mean_2_S, mean_3_S)\n",
    "print(mean_1_L, mean_2_L, mean_3_L)\n",
    "print(max_1_S, max_2_S, max_3_S)\n",
    "print(max_1_L, max_2_L, max_3_L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mscproj]",
   "language": "python",
   "name": "conda-env-mscproj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
